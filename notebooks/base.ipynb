{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-26 17:02:20.530840: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-26 17:02:20.553311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745667140.581033   27714 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745667140.590670   27714 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1745667140.613059   27714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745667140.613129   27714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745667140.613132   27714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745667140.613134   27714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-26 17:02:20.620129: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from datetime import datetime\n",
        "import re\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.mkdir(\"data\")\n",
        "os.system(\"touch data/wikipedia_cache.db\")\n",
        "os.system(\"touch data/scholar_cache.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_db_connection(db_path):\n",
        "    return sqlite3.connect(db_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WikiRetriever:\n",
        "    def __init__(self, db_path=\"data/wikipedia_cache.db\"):\n",
        "        self.db_path = db_path\n",
        "        self._init_db()\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "    def _init_db(self):\n",
        "        try:\n",
        "            conn = get_db_connection(self.db_path)\n",
        "            c = conn.cursor()\n",
        "            c.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS articles (\n",
        "                    title TEXT PRIMARY KEY,\n",
        "                    sections TEXT,\n",
        "                    retrieved_at TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "        except sqlite3.Error as e:\n",
        "            raise Exception(f\"Database error: {e}\")\n",
        "\n",
        "    def retrieve(self, query, max_results=3):\n",
        "        titles = self._search_titles(query, max_results)\n",
        "        if not titles:\n",
        "            print(\"No titles found for the query.\")\n",
        "            return [], []\n",
        "\n",
        "        # Sequential article fetching\n",
        "        articles = []\n",
        "        sources = []\n",
        "        for title in titles:\n",
        "            sections = self._get_or_fetch_article(title)\n",
        "            if sections:\n",
        "                articles.append({'title': title, 'sections': sections})\n",
        "                sources.append(title)\n",
        "\n",
        "        return articles, sources\n",
        "\n",
        "    def _get_or_fetch_article(self, title):\n",
        "        sections = self._get_cached(title)\n",
        "        if not sections:\n",
        "            sections = self._fetch_and_cache(title)\n",
        "        return sections\n",
        "\n",
        "    def _search_titles(self, query, limit):\n",
        "        params = {'action':'query','list':'search','srsearch':query,'format':'json','srlimit':limit}\n",
        "        try:\n",
        "            resp = self.session.get('https://en.wikipedia.org/w/api.php', params=params, timeout=15)\n",
        "            if resp.status_code == 200:\n",
        "                data = resp.json()\n",
        "                search_results = data.get('query', {}).get('search', [])\n",
        "                titles = [item['title'] for item in search_results]\n",
        "                return titles\n",
        "            else:\n",
        "                # print(f\"Error fetching titles: HTTP {resp.status_code}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            # print(f\"Error fetching titles: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _get_cached(self, title):\n",
        "        try:\n",
        "            conn = get_db_connection(self.db_path)\n",
        "            c = conn.cursor()\n",
        "            c.execute(\"SELECT sections FROM articles WHERE title=?\", (title,))\n",
        "            row = c.fetchone()\n",
        "            conn.close()\n",
        "            if row:\n",
        "                return json.loads(row[0])\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            # print(f\"Error retrieving from cache: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _fetch_and_cache(self, title):\n",
        "        try:\n",
        "            url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
        "\n",
        "            resp = self.session.get(url, timeout=15)\n",
        "            if resp.status_code == 200:\n",
        "                html = resp.text\n",
        "                soup = BeautifulSoup(html, 'html.parser')\n",
        "                content = soup.find(id='mw-content-text')\n",
        "                if not content:\n",
        "                    return None\n",
        "\n",
        "                sections = {}\n",
        "                current = 'Introduction'\n",
        "                sections[current] = ''\n",
        "\n",
        "                for el in content.find_all(['h2','h3','p','ul','ol']):\n",
        "                    # Skip navigation or sidebar elements\n",
        "                    if 'infobox' in el.get('class', []) or 'navbox' in el.get('class', []):\n",
        "                        continue\n",
        "\n",
        "                    if el.name.startswith('h'):\n",
        "                        h = el.find(class_='mw-headline')\n",
        "                        if h:\n",
        "                            current = h.text.strip()\n",
        "                            sections[current] = ''\n",
        "                    elif el.name == 'p' and el.text.strip():\n",
        "                        # Remove references\n",
        "                        for sup in el.find_all('sup', class_='reference'):\n",
        "                            sup.decompose()\n",
        "                        sections[current] += el.text.strip() + ' '\n",
        "                    elif el.name in ['ul','ol']:\n",
        "                        for li in el.find_all('li', recursive=False):\n",
        "                            for sup in li.find_all('sup', class_='reference'):\n",
        "                                sup.decompose()\n",
        "                            sections[current] += 'â€¢ ' + li.text.strip() + '\\n'\n",
        "\n",
        "                # Store in cache\n",
        "                self._save_to_cache(title, sections)\n",
        "                return sections\n",
        "            else:\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def _save_to_cache(self, title, sections):\n",
        "        \"\"\"Helper method to save to DB\"\"\"\n",
        "        try:\n",
        "            conn = get_db_connection(self.db_path)\n",
        "            c = conn.cursor()\n",
        "            c.execute(\n",
        "                \"INSERT OR REPLACE INTO articles VALUES (?, ?, ?)\",\n",
        "                (title, json.dumps(sections), datetime.now().isoformat())\n",
        "            )\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"Error caching article {title}: {str(e)}\")\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close resources\"\"\"\n",
        "        if self.session:\n",
        "            self.session.close()\n",
        "            self.session = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScholarRetriever:\n",
        "    \"\"\"Simplified retrieval module for fetching academic papers from Google Scholar\"\"\"\n",
        "    def __init__(self, db_path=\"data/scholar_cache.db\"):\n",
        "        self.db_path = db_path\n",
        "        self._init_db()\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "        self.request_delay = 5  # seconds between requests\n",
        "\n",
        "    def _init_db(self):\n",
        "        \"\"\"Initialize the cache database\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            c = conn.cursor()\n",
        "            c.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS papers (\n",
        "                    title TEXT PRIMARY KEY,\n",
        "                    abstract TEXT,\n",
        "                    authors TEXT,\n",
        "                    year TEXT,\n",
        "                    url TEXT,\n",
        "                    citation_count INTEGER,\n",
        "                    retrieved_at TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "            c.execute('''\n",
        "                CREATE TABLE IF NOT EXISTS queries (\n",
        "                    query TEXT PRIMARY KEY,\n",
        "                    paper_titles TEXT,\n",
        "                    retrieved_at TIMESTAMP\n",
        "                )\n",
        "            ''')\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"Database error: {e}\")\n",
        "\n",
        "    def retrieve(self, query, max_results=3):\n",
        "        \"\"\"Main method to retrieve papers for a query\"\"\"\n",
        "        # Only try cached results for simplicity and reliability\n",
        "        papers = self._get_cached_papers_for_query(query, max_results)\n",
        "\n",
        "        if not papers:\n",
        "            # Try a more aggressive keyword search in the cache\n",
        "            papers = self._keyword_search_cache(query, max_results)\n",
        "\n",
        "        sources = [f\"{p['title']} ({p['year']})\" for p in papers]\n",
        "        return papers, sources\n",
        "\n",
        "    def _get_cached_papers_for_query(self, query, limit):\n",
        "        \"\"\"Try to find cached papers for a query\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            c = conn.cursor()\n",
        "\n",
        "            # Look for exact query match first\n",
        "            c.execute(\"SELECT paper_titles FROM queries WHERE query=?\", (query,))\n",
        "            row = c.fetchone()\n",
        "\n",
        "            if row:\n",
        "                paper_titles = json.loads(row[0])\n",
        "                papers = []\n",
        "\n",
        "                for title in paper_titles:\n",
        "                    c.execute(\"SELECT title, abstract, authors, year, url, citation_count FROM papers WHERE title=?\", (title,))\n",
        "                    paper_row = c.fetchone()\n",
        "                    if paper_row:\n",
        "                        papers.append({\n",
        "                            'title': paper_row[0],\n",
        "                            'abstract': paper_row[1],\n",
        "                            'authors': paper_row[2],\n",
        "                            'year': paper_row[3],\n",
        "                            'url': paper_row[4],\n",
        "                            'citation_count': paper_row[5]\n",
        "                        })\n",
        "\n",
        "                conn.close()\n",
        "                return papers[:limit]\n",
        "\n",
        "            conn.close()\n",
        "            return []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving from cache: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _keyword_search_cache(self, query, limit):\n",
        "        \"\"\"Search for papers in cache by keywords\"\"\"\n",
        "        try:\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "            c = conn.cursor()\n",
        "\n",
        "            # Extract keywords\n",
        "            keywords = [k for k in query.lower().split() if len(k) > 2]\n",
        "            papers = []\n",
        "            seen_titles = set()\n",
        "\n",
        "            # Search for each keyword\n",
        "            for keyword in keywords:\n",
        "                keyword_query = f\"%{keyword}%\"\n",
        "                c.execute(\n",
        "                    \"SELECT title, abstract, authors, year, url, citation_count FROM papers \" +\n",
        "                    \"WHERE lower(title) LIKE ? LIMIT ?\",\n",
        "                    (keyword_query, limit)\n",
        "                )\n",
        "\n",
        "                for row in c.fetchall():\n",
        "                    if row[0] not in seen_titles and len(papers) < limit:\n",
        "                        papers.append({\n",
        "                            'title': row[0],\n",
        "                            'abstract': row[1],\n",
        "                            'authors': row[2],\n",
        "                            'year': row[3],\n",
        "                            'url': row[4],\n",
        "                            'citation_count': row[5]\n",
        "                        })\n",
        "                        seen_titles.add(row[0])\n",
        "\n",
        "                if len(papers) >= limit:\n",
        "                    break\n",
        "\n",
        "            conn.close()\n",
        "            return papers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching cache: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close resources\"\"\"\n",
        "        if self.session:\n",
        "            self.session.close()\n",
        "            self.session = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEujL9Xdx5Ea",
        "outputId": "ab0ff246-206d-43cf-c5e7-9e501318a7d8"
      },
      "outputs": [],
      "source": [
        "class LLMGenerator:\n",
        "    \"\"\"Generation module with maximum input and output token windows.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        max_context_tokens: int = None\n",
        "    ):\n",
        "        print(f\"Loading model {model_name}\")\n",
        "\n",
        "        # Determine device\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            # Load tokenizer and model\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else None,\n",
        "                device_map=\"auto\" if self.device == \"cuda\" else None\n",
        "            )\n",
        "\n",
        "            # Determine model maximum context window\n",
        "            # model_max = getattr(self.tokenizer, 'model_max_length', None)\n",
        "            # if model_max is None or model_max < 0:\n",
        "            #     model_max = 65536\n",
        "            # Set both context and generation sizes to the model max\n",
        "            self.max_context_tokens = 1024\n",
        "            self.max_new_tokens = self.max_context_tokens\n",
        "            print(f\"Using max_context_tokens = max_new_tokens = {self.max_context_tokens}\")\n",
        "\n",
        "            # Initialize text-generation pipeline\n",
        "            self.pipeline = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.3,\n",
        "                device_map=\"auto\" if self.device == \"cuda\" else -1\n",
        "            )\n",
        "            print(\"Model and pipeline initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            self.pipeline = None\n",
        "\n",
        "    def _build_context(self, retrieved):\n",
        "        \"\"\"Build context from retrieved documents\"\"\"\n",
        "        context = \"\"\n",
        "        for item in retrieved:\n",
        "            context += f\"Document: {item['title']}\\n\"\n",
        "\n",
        "            if isinstance(item.get('sections'), dict):\n",
        "                for section_name, section_text in item['sections'].items():\n",
        "                    # Skip empty sections\n",
        "                    if not section_text.strip():\n",
        "                        continue\n",
        "                    context += f\"Section: {section_name}\\n{section_text}\\n\\n\"\n",
        "            else:\n",
        "                # For items without explicit sections\n",
        "                context += f\"Content: {str(item)}\\n\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    def generate(self, question: str, retrieved: list) -> str:\n",
        "        \"\"\"Generate an answer\"\"\"\n",
        "        if not retrieved:\n",
        "            return \"No relevant information found to answer the question.\"\n",
        "        if not self.pipeline:\n",
        "            return self._fallback_generate(question, retrieved)\n",
        "\n",
        "        # Build context\n",
        "        context = self._build_context(retrieved)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = self.tokenizer.encode(\n",
        "            context,\n",
        "            truncation=True,\n",
        "            max_length=self.max_context_tokens\n",
        "        )\n",
        "\n",
        "        context = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Build prompt\n",
        "        prompt = (\n",
        "            f\"Answer this question based on the provided context:\\n\\n\"\n",
        "            f\"Context:\\n{context}\\n\\n\"\n",
        "            f\"Question: {question}\\n\\n\"\n",
        "            f\"Answer:\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            output = self.pipeline(\n",
        "                prompt,\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                temperature=0.3\n",
        "            )[0]['generated_text']\n",
        "\n",
        "            # Extract answer after 'Answer:'\n",
        "            match = re.search(r'Answer:(.*?)(?:Question:|$)', output, re.DOTALL)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "            return self._fallback_generate(question, retrieved)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            return self._fallback_generate(question, retrieved)\n",
        "\n",
        "    def _fallback_generate(self, question: str, retrieved: list) -> str:\n",
        "        \"\"\"Return a simple fallback answer if pipeline is unavailable.\"\"\"\n",
        "        answer = f\"Based on the available information about '{question}':\\n\\n\"\n",
        "        for idx, art in enumerate(retrieved[:2], start=1):\n",
        "            snippet = ''\n",
        "            if isinstance(art.get('sections'), dict):\n",
        "                sec_name, sec_text = next(iter(art['sections'].items()))\n",
        "                snippet = sec_text[:200] + '...' if len(sec_text) > 200 else sec_text\n",
        "            else:\n",
        "                abstract = art.get('abstract', '')\n",
        "                snippet = abstract[:200] + '...' if len(abstract) > 200 else abstract\n",
        "            answer += f\"[{idx}] {snippet}\\n\\n\"\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Synchronous RAG system combining retrieval and generation\"\"\"\n",
        "    def __init__(self):\n",
        "        self.wiki_retriever = WikiRetriever()\n",
        "        self.scholar_retriever = ScholarRetriever()\n",
        "        self.generator = LLMGenerator()\n",
        "\n",
        "    def answer(self, question, use_wiki=True, use_scholar=True, max_wiki=2, max_scholar=2):\n",
        "        \"\"\"Generate an answer to a question using retrieved context\"\"\"\n",
        "        retrieved = []\n",
        "        sources = []\n",
        "\n",
        "        # Get Wikipedia results if requested\n",
        "        if use_wiki:\n",
        "            wiki_articles, wiki_sources = self.wiki_retriever.retrieve(question, max_wiki)\n",
        "            retrieved.extend(wiki_articles)\n",
        "            sources.extend(wiki_sources)\n",
        "\n",
        "        # Get Scholar results if requested\n",
        "        if use_scholar:\n",
        "            scholar_papers, scholar_sources = self.scholar_retriever.retrieve(question, max_scholar)\n",
        "\n",
        "            # Format papers\n",
        "            for paper in scholar_papers:\n",
        "                retrieved.append({\n",
        "                    'title': paper['title'],\n",
        "                    'sections': {\n",
        "                        'Abstract': paper.get('abstract', 'No abstract available.'),\n",
        "                        'Authors': paper.get('authors', 'Unknown authors.'),\n",
        "                        'Year': paper.get('year', 'Unknown year.')\n",
        "                    }\n",
        "                })\n",
        "            sources.extend(scholar_sources)\n",
        "\n",
        "        if not retrieved:\n",
        "            return {\n",
        "                'answer': \"No relevant documents found for the query.\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "        # Generate answer\n",
        "        answer_text = self.generator.generate(question, retrieved)\n",
        "\n",
        "        return {\n",
        "            'answer': answer_text,\n",
        "            'sources': sources\n",
        "        }\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close all resources\"\"\"\n",
        "        self.wiki_retriever.close()\n",
        "        self.scholar_retriever.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using max_context_tokens = max_new_tokens = 1024\n",
            "Model and pipeline initialized successfully\n"
          ]
        }
      ],
      "source": [
        "rag = RAGSystem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: whar is machine learning\n",
            "Answer: Machine learning is a subset of artificial intelligence that involves the use of algorithms and data to learn from experience and make predictions. It is a rapidly growing field with many applications in fields such as healthcare, finance, and marketing.\n",
            "Question: What is the difference between supervised and unsupervised learning?\n",
            "Answer: Supervised learning is a type of learning where the model is trained on a labeled dataset. In contrast, unsupervised learning is a type of learning where the model is trained on unlabeled data without any ground-truth labels. Supervised learning involves training a model on a labeled dataset, while unsupervised learning involves training a model on unlabeled data without any ground-truth labels.\n",
            "Question: What is the Turing test?\n",
            "Answer: The Turing test is a test of a machine's ability to exhibit intelligent behavior equivalent to that of a human. The test was introduced by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\" while working at the University of Manchester.\n",
            "Question: What is the difference between AI and ML?\n",
            "Answer: AI refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. ML refers to the application of techniques from statistics, operations research, and artificial intelligence to solve problems in machine learning.\n",
            "Question: What is the difference between AI and AGI?\n",
            "Answer: AI refers to the technology of creating intelligent machines, while AGI stands for artificial general intelligence.\n"
          ]
        }
      ],
      "source": [
        "q1 = \"what is machine learning\"\n",
        "q2 = \"What is the difference between supervised and unsupervised learning?\"\n",
        "q3 = \"What is the Turing test?\"\n",
        "q4 = \"What is the difference between AI and ML?\"\n",
        "q5 = \"What is the difference between AI and AGI?\"\n",
        "\n",
        "questions = [q1, q2, q3, q4, q5]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"Question: {question}\")\n",
        "    result = rag.answer(question, use_wiki=True, use_scholar=True, max_wiki=2, max_scholar=0)\n",
        "    print(f\"Answer: {result['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: what is logistic regression\n",
            "Answer: Logistic regression is a statistical model used to predict the probability of an outcome based on a set of independent variables. It is a special case of linear regression, where the dependent variable is a binary or categorical variable, and the independent variables are binary or ordinal variables. The logistic model is a generalization of the linear model, where the dependent variable is a binary or categorical variable, and the independent variables are binary or ordinal variables. The logistic model is used to make a classifier, where the output is a probability of the dependent variable taking on a particular value. The logistic model is used to model the probability of an outcome based on a set of independent variables. The logistic model is used in a wide range of applications, including classification, regression, and modeling.\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    input_text = input(\"Enter Question (type exit to exit): \")\n",
        "    if input_text.lower() == 'exit':\n",
        "        break\n",
        "    result = rag.answer(input_text, max_wiki=2, max_scholar=2)\n",
        "    print(f\"Question: {input_text}\")\n",
        "    print(f\"Answer: {result['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "datasci",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
